{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Grade Prediction Model\n",
    "\n",
    "## Authors:\n",
    "- Samuel Nicklaus\n",
    "- Jacob Kindle\n",
    "- Lane Swartzendruber\n",
    "- Alejandro Mirafuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, our team focuses on predicting a student's academic performance based on various personal, academic, and social factors using **classification**. We aim to classify students into different grade categories (A, B, C, etc.) based on inputs such as age, sex, high school type, participation in extracurricular activities, and more.\n",
    "\n",
    "*Data Source*: [Here](https://www.kaggle.com/datasets/jacksondivakarr/student-classification-dataset?select=student.csv)\n",
    "\n",
    "*Data Source Description (Via Kaggle):*\n",
    "\n",
    "This dataset encompasses various aspects related to student performance. Each entry is uniquely identified by an 'Id'. The dataset includes demographic information such as 'Student_Age' and 'Sex'. 'High_School_Type' categorizes the type of high school attended, while 'Scholarship' indicates whether the student has a scholarship. Details about 'Additional_Work' and involvement in 'Sports_activity' provide insights into extracurricular commitments.\n",
    "\n",
    "'Transportation' outlines the mode of commuting for each student. Academic aspects are captured through 'Weekly_Study_Hours', 'Attendance', and evaluations of 'Reading', 'Notes', and 'Listening_in_Class'. The culmination of these factors is reflected in the 'Grade' column, providing a comprehensive overview of student performance. This dataset serves as a valuable resource for exploring the multifaceted dynamics influencing academic outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import (datasets, neighbors,\n",
    "                     naive_bayes,\n",
    "                     model_selection as skms,\n",
    "                     linear_model, dummy,\n",
    "                     metrics,\n",
    "                     pipeline,\n",
    "                     preprocessing as skpre)\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "- Student_Age: The age of the student.\n",
    "\n",
    "\n",
    "- Sex: The gender of the student (e.g., Male, Female).\n",
    "\n",
    "\n",
    "- High_School_Type: The type of high school the student attended (e.g., State, Private, Other).\n",
    "\n",
    "\n",
    "- Scholarship: Information on whether the student received a scholarship (e.g., percentages indicating the scholarship amount).\n",
    "\n",
    "\n",
    "- Additional_Work: Indicates whether the student has additional work or responsibilities outside of school (Yes/No).\n",
    "\n",
    "\n",
    "- Sports_activity: Indicates whether the student participates in sports activities (Yes/No).\n",
    "\n",
    "\n",
    "- Transportation: The mode of transportation used by the student to get to school (e.g., Bus, Private).\n",
    "\n",
    "\n",
    "- Weekly_Study_Hours: The number of hours the student spends studying each week.\n",
    "\n",
    "\n",
    "- Attendance: The regularity of the student's attendance (e.g., Always, Never).\n",
    "\n",
    "\n",
    "- Reading: Indicates whether the student engages in reading activities outside of the required curriculum (Yes/No).\n",
    "\n",
    "\n",
    "- Notes: Indicates whether the student takes notes during classes or while studying (Yes/No).\n",
    "\n",
    "\n",
    "- Listening_in_Class: Indicates whether the student actively listens in class (Yes/No).\n",
    "\n",
    "\n",
    "- Project_work: Indicates whether the student participates in project work (Yes/No).\n",
    "\n",
    "\n",
    "- Grade: The grade the student received, which could be in a variety of formats (e.g., AA, BB, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Student_Age', 'Weekly_Study_Hours', 'Grade', 'Sex_Female', 'Sex_Male',\n",
      "       'High_School_Type_Other', 'High_School_Type_Private',\n",
      "       'High_School_Type_State', 'Scholarship_100%', 'Scholarship_25%',\n",
      "       'Scholarship_50%', 'Scholarship_75%', 'Additional_Work_No',\n",
      "       'Additional_Work_Yes', 'Sports_activity_No', 'Sports_activity_Yes',\n",
      "       'Transportation_Bus', 'Transportation_Private', 'Attendance_Always',\n",
      "       'Attendance_Never', 'Attendance_Sometimes', 'Reading_No', 'Reading_Yes',\n",
      "       'Notes_No', 'Notes_Yes', 'Listening_in_Class_No',\n",
      "       'Listening_in_Class_Yes', 'Project_work_No', 'Project_work_Yes'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student_Age</th>\n",
       "      <th>Weekly_Study_Hours</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Sex_Female</th>\n",
       "      <th>Sex_Male</th>\n",
       "      <th>High_School_Type_Other</th>\n",
       "      <th>High_School_Type_Private</th>\n",
       "      <th>High_School_Type_State</th>\n",
       "      <th>Scholarship_100%</th>\n",
       "      <th>Scholarship_25%</th>\n",
       "      <th>...</th>\n",
       "      <th>Attendance_Never</th>\n",
       "      <th>Attendance_Sometimes</th>\n",
       "      <th>Reading_No</th>\n",
       "      <th>Reading_Yes</th>\n",
       "      <th>Notes_No</th>\n",
       "      <th>Notes_Yes</th>\n",
       "      <th>Listening_in_Class_No</th>\n",
       "      <th>Listening_in_Class_Yes</th>\n",
       "      <th>Project_work_No</th>\n",
       "      <th>Project_work_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student_Age  Weekly_Study_Hours Grade  Sex_Female  Sex_Male  \\\n",
       "0           21                   0     A         0.0       1.0   \n",
       "1           20                   0     A         0.0       1.0   \n",
       "2           21                   2     A         0.0       1.0   \n",
       "3           18                   2     A         1.0       0.0   \n",
       "4           22                  12     A         0.0       1.0   \n",
       "\n",
       "   High_School_Type_Other  High_School_Type_Private  High_School_Type_State  \\\n",
       "0                     1.0                       0.0                     0.0   \n",
       "1                     1.0                       0.0                     0.0   \n",
       "2                     0.0                       0.0                     1.0   \n",
       "3                     0.0                       1.0                     0.0   \n",
       "4                     0.0                       1.0                     0.0   \n",
       "\n",
       "   Scholarship_100%  Scholarship_25%  ...  Attendance_Never  \\\n",
       "0               0.0              0.0  ...               0.0   \n",
       "1               0.0              0.0  ...               0.0   \n",
       "2               0.0              0.0  ...               1.0   \n",
       "3               0.0              0.0  ...               0.0   \n",
       "4               0.0              0.0  ...               0.0   \n",
       "\n",
       "   Attendance_Sometimes  Reading_No  Reading_Yes  Notes_No  Notes_Yes  \\\n",
       "0                   0.0         0.0          1.0       0.0        1.0   \n",
       "1                   0.0         0.0          1.0       1.0        0.0   \n",
       "2                   0.0         1.0          0.0       1.0        0.0   \n",
       "3                   0.0         1.0          0.0       0.0        1.0   \n",
       "4                   0.0         0.0          1.0       1.0        0.0   \n",
       "\n",
       "   Listening_in_Class_No  Listening_in_Class_Yes  Project_work_No  \\\n",
       "0                    1.0                     0.0              1.0   \n",
       "1                    0.0                     1.0              0.0   \n",
       "2                    1.0                     0.0              0.0   \n",
       "3                    1.0                     0.0              1.0   \n",
       "4                    0.0                     1.0              0.0   \n",
       "\n",
       "   Project_work_Yes  \n",
       "0               0.0  \n",
       "1               1.0  \n",
       "2               1.0  \n",
       "3               0.0  \n",
       "4               1.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data, drop unneeded columns\n",
    "df = pd.read_csv('student.csv')\n",
    "df.drop(['Unnamed: 0', 'Id'], axis=1, inplace=True)\n",
    "\n",
    "# Use one-hot encoding for categorical variables\n",
    "categorical_vars = ['Sex', 'High_School_Type', 'Scholarship', 'Additional_Work', 'Sports_activity', 'Transportation', 'Attendance', 'Reading', 'Notes', 'Listening_in_Class', 'Project_work']\n",
    "\n",
    "one_hot_encoder = skpre.OneHotEncoder(sparse_output=False)\n",
    "encoded_vars = one_hot_encoder.fit_transform(df[categorical_vars])\n",
    "encoded_vars_df = pd.DataFrame(encoded_vars, columns=one_hot_encoder.get_feature_names_out(categorical_vars))\n",
    "\n",
    "# Drop original categorical variables and some columns that are not needed\n",
    "df.drop(categorical_vars, axis=1, inplace=True)\n",
    "grade_mapping = {\n",
    "    'AA': 'A', 'BA': 'B', 'BB': 'B', 'BC': 'B', 'CC': 'C', 'CB': 'C', 'DC': 'D', 'DD': 'D', 'Fail': 'F'\n",
    "}\n",
    "df['Grade'] = df['Grade'].map(grade_mapping)\n",
    "encoded_vars_df.drop(['Attendance_3','Notes_6','Listening_in_Class_6','Scholarship_nan'], axis=1, inplace=True)\n",
    "df_encoded = pd.concat([df, encoded_vars_df], axis=1)\n",
    "\n",
    "# Split data\n",
    "X = df_encoded.drop('Grade', axis=1)  # Features\n",
    "y = df_encoded['Grade']  # Target variable\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = skms.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = skms.train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "# Results in 60% train, 20% validation, 20% test sets\n",
    "\n",
    "# Print columns and final dataframe\n",
    "print(df_encoded.columns)\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='Grade', order=df['Grade'].value_counts().index)\n",
    "plt.title('Distribution of Grades')\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "df_for_heatmap = df_encoded.copy()\n",
    "df_for_heatmap['Grade'] = df_for_heatmap['Grade']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13793103448275862\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.1724137931034483\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.3793103448275862\n",
      "0.13793103448275862\n",
      "0.20689655172413793\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.4482758620689655\n",
      "0.3448275862068966\n",
      "0.3793103448275862\n",
      "0.3793103448275862\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.10344827586206896\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.1724137931034483\n",
      "0.1724137931034483\n",
      "0.4482758620689655\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.3103448275862069\n",
      "0.41379310344827586\n",
      "0.3448275862068966\n",
      "0.3793103448275862\n",
      "0.3793103448275862\n",
      "0.3103448275862069\n",
      "0.1724137931034483\n",
      "0.3448275862068966\n",
      "0.1724137931034483\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.13793103448275862\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.1724137931034483\n",
      "0.27586206896551724\n",
      "0.20689655172413793\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.41379310344827586\n",
      "0.27586206896551724\n",
      "0.20689655172413793\n",
      "0.1724137931034483\n",
      "0.1724137931034483\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.3793103448275862\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.13793103448275862\n",
      "0.1724137931034483\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.3793103448275862\n",
      "0.3448275862068966\n",
      "0.4827586206896552\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.20689655172413793\n",
      "0.3793103448275862\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.3793103448275862\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.3793103448275862\n",
      "0.27586206896551724\n",
      "0.20689655172413793\n",
      "0.41379310344827586\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.41379310344827586\n",
      "0.3448275862068966\n",
      "0.3793103448275862\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.5517241379310345\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.4827586206896552\n",
      "0.3793103448275862\n",
      "0.3793103448275862\n",
      "0.3793103448275862\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.13793103448275862\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.13793103448275862\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.13793103448275862\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.10344827586206896\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.41379310344827586\n",
      "0.3448275862068966\n",
      "0.4482758620689655\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.10344827586206896\n",
      "0.10344827586206896\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.27586206896551724\n",
      "0.41379310344827586\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.27586206896551724\n",
      "0.41379310344827586\n",
      "0.41379310344827586\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.13793103448275862\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.3793103448275862\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.1724137931034483\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.41379310344827586\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.13793103448275862\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.20689655172413793\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.20689655172413793\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.4827586206896552\n",
      "0.27586206896551724\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.4482758620689655\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.13793103448275862\n",
      "0.10344827586206896\n",
      "0.1724137931034483\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.13793103448275862\n",
      "0.27586206896551724\n",
      "0.3793103448275862\n",
      "0.27586206896551724\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.13793103448275862\n",
      "0.20689655172413793\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.1724137931034483\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.10344827586206896\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.3793103448275862\n",
      "0.41379310344827586\n",
      "0.3448275862068966\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.13793103448275862\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.13793103448275862\n",
      "0.27586206896551724\n",
      "0.13793103448275862\n",
      "0.2413793103448276\n",
      "0.3793103448275862\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.27586206896551724\n",
      "0.20689655172413793\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.1724137931034483\n",
      "0.3448275862068966\n",
      "0.1724137931034483\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.10344827586206896\n",
      "0.3448275862068966\n",
      "0.3448275862068966\n",
      "0.3448275862068966\n",
      "0.3448275862068966\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.5517241379310345\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.41379310344827586\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.3793103448275862\n",
      "0.41379310344827586\n",
      "0.27586206896551724\n",
      "0.3793103448275862\n",
      "0.4827586206896552\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.41379310344827586\n",
      "0.1724137931034483\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.3793103448275862\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.3793103448275862\n",
      "0.3448275862068966\n",
      "0.3793103448275862\n",
      "0.20689655172413793\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.3448275862068966\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.3793103448275862\n",
      "0.41379310344827586\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.41379310344827586\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.3793103448275862\n",
      "0.3448275862068966\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.3448275862068966\n",
      "0.3793103448275862\n",
      "0.5172413793103449\n",
      "0.5172413793103449\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.3448275862068966\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.41379310344827586\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.41379310344827586\n",
      "0.4482758620689655\n",
      "0.5517241379310345\n",
      "0.41379310344827586\n",
      "0.5862068965517241\n",
      "0.4827586206896552\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.3448275862068966\n",
      "0.41379310344827586\n",
      "0.41379310344827586\n",
      "0.41379310344827586\n",
      "0.3103448275862069\n",
      "0.4827586206896552\n",
      "0.3793103448275862\n",
      "0.5172413793103449\n",
      "0.3448275862068966\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.1724137931034483\n",
      "0.13793103448275862\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.27586206896551724\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.10344827586206896\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.1724137931034483\n",
      "0.1724137931034483\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.13793103448275862\n",
      "0.20689655172413793\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.13793103448275862\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.06896551724137931\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.10344827586206896\n",
      "0.3448275862068966\n",
      "0.3793103448275862\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.3793103448275862\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.1724137931034483\n",
      "0.5172413793103449\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.3793103448275862\n",
      "0.41379310344827586\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.3793103448275862\n",
      "0.2413793103448276\n",
      "0.10344827586206896\n",
      "0.27586206896551724\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.13793103448275862\n",
      "0.27586206896551724\n",
      "0.13793103448275862\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.41379310344827586\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.13793103448275862\n",
      "0.3448275862068966\n",
      "0.13793103448275862\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.3793103448275862\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.3793103448275862\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.41379310344827586\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.4482758620689655\n",
      "0.41379310344827586\n",
      "0.4827586206896552\n",
      "0.4482758620689655\n",
      "0.4482758620689655\n",
      "0.3448275862068966\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.2413793103448276\n",
      "0.13793103448275862\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.3448275862068966\n",
      "0.41379310344827586\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.1724137931034483\n",
      "0.1724137931034483\n",
      "0.13793103448275862\n",
      "0.1724137931034483\n",
      "0.10344827586206896\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.06896551724137931\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n",
      "0.13793103448275862\n",
      "0.27586206896551724\n",
      "0.13793103448275862\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.1724137931034483\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.10344827586206896\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.1724137931034483\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.10344827586206896\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.3448275862068966\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.1724137931034483\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.13793103448275862\n",
      "0.13793103448275862\n",
      "0.1724137931034483\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.27586206896551724\n",
      "0.20689655172413793\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.20689655172413793\n",
      "0.13793103448275862\n",
      "0.41379310344827586\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.4482758620689655\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.3793103448275862\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.41379310344827586\n",
      "0.5517241379310345\n",
      "0.4827586206896552\n",
      "0.4827586206896552\n",
      "0.4482758620689655\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.13793103448275862\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.4482758620689655\n",
      "0.27586206896551724\n",
      "0.4482758620689655\n",
      "0.3448275862068966\n",
      "0.41379310344827586\n",
      "0.41379310344827586\n",
      "0.41379310344827586\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.4482758620689655\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.20689655172413793\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.3448275862068966\n",
      "0.3793103448275862\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.27586206896551724\n",
      "0.1724137931034483\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.1724137931034483\n",
      "0.3793103448275862\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.4482758620689655\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.13793103448275862\n",
      "0.3448275862068966\n",
      "0.3793103448275862\n",
      "0.20689655172413793\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.4827586206896552\n",
      "0.41379310344827586\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.3103448275862069\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.1724137931034483\n",
      "0.3448275862068966\n",
      "0.27586206896551724\n",
      "0.20689655172413793\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.41379310344827586\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.27586206896551724\n",
      "0.4482758620689655\n",
      "0.3793103448275862\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.41379310344827586\n",
      "0.41379310344827586\n",
      "0.4482758620689655\n",
      "0.4827586206896552\n",
      "0.4482758620689655\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.41379310344827586\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.3448275862068966\n",
      "0.20689655172413793\n",
      "0.3103448275862069\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.3793103448275862\n",
      "0.41379310344827586\n",
      "0.3448275862068966\n",
      "0.3448275862068966\n",
      "0.41379310344827586\n",
      "0.3448275862068966\n",
      "0.4827586206896552\n",
      "0.4482758620689655\n",
      "0.2413793103448276\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.41379310344827586\n",
      "0.2413793103448276\n",
      "0.3448275862068966\n",
      "0.3103448275862069\n",
      "0.27586206896551724\n",
      "0.41379310344827586\n",
      "0.3448275862068966\n",
      "0.4482758620689655\n",
      "0.4482758620689655\n",
      "0.5172413793103449\n",
      "0.4482758620689655\n",
      "0.4827586206896552\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.5517241379310345\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.2413793103448276\n",
      "0.20689655172413793\n",
      "0.20689655172413793\n",
      "0.3793103448275862\n",
      "0.3103448275862069\n",
      "0.3793103448275862\n",
      "0.3448275862068966\n",
      "0.3448275862068966\n",
      "0.4482758620689655\n",
      "0.3103448275862069\n",
      "0.41379310344827586\n",
      "0.3448275862068966\n",
      "0.4482758620689655\n",
      "0.20689655172413793\n",
      "0.3448275862068966\n",
      "0.2413793103448276\n",
      "0.1724137931034483\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m X_validation_sub \u001b[38;5;241m=\u001b[39m X_validation[selected_features]\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Fitting the model\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m fit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m predictions \u001b[38;5;241m=\u001b[39m fit\u001b[38;5;241m.\u001b[39mpredict(X_validation_sub)\n\u001b[0;32m     41\u001b[0m score \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39maccuracy_score(y_validation, predictions)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "best_score = 0\n",
    "best_model = None\n",
    "best_features = None\n",
    "models = [\n",
    "    {\"name\": \"KNN-n=1\", \"model\": neighbors.KNeighborsClassifier(n_neighbors=1)},\n",
    "    {\"name\": \"KNN-n=2\", \"model\": neighbors.KNeighborsClassifier(n_neighbors=2)},\n",
    "    {\"name\": \"KNN-n=3\", \"model\": neighbors.KNeighborsClassifier(n_neighbors=3)},\n",
    "    {\"name\": \"KNN-n=4\", \"model\": neighbors.KNeighborsClassifier(n_neighbors=4)},\n",
    "    {\"name\": \"KNN-n=9\", \"model\": neighbors.KNeighborsClassifier(n_neighbors=9)},\n",
    "    {\"name\": \"GaussianNB\", \"model\": naive_bayes.GaussianNB()},\n",
    "]\n",
    "feature_groups = {\n",
    "    'Weekly_Study_Hours': ['Weekly_Study_Hours'],\n",
    "    'Listening_in_Class': ['Listening_in_Class_No', 'Listening_in_Class_Yes'],\n",
    "    'Sports_activity': ['Sports_activity_No', 'Sports_activity_Yes'],\n",
    "    'Sex': ['Sex_Female', 'Sex_Male'],\n",
    "    'High_School_Type': ['High_School_Type_Private', 'High_School_Type_State', 'High_School_Type_Other'],\n",
    "    'Scholarship': ['Scholarship_25%', 'Scholarship_50%', 'Scholarship_75%', 'Scholarship_100%'],\n",
    "    'Additional_Work': ['Additional_Work_No', 'Additional_Work_Yes'],\n",
    "    'Transportation': ['Transportation_Bus', 'Transportation_Private'],\n",
    "    'Reading': ['Reading_No', 'Reading_Yes'],\n",
    "    'Notes': ['Notes_No', 'Notes_Yes'],\n",
    "    'Project_work': ['Project_work_No', 'Project_work_Yes'],\n",
    "}\n",
    "# Iterating through each model\n",
    "for model in models:\n",
    "    # Trying every combination of feature groups\n",
    "    for L in range(1, len(feature_groups) + 1):\n",
    "        for subset in combinations(feature_groups.keys(), L):\n",
    "            selected_features = [feature for group in subset for feature in feature_groups[group]]\n",
    "            # Selecting the subset of features\n",
    "            X_train_sub = X_train[selected_features]\n",
    "            X_validation_sub = X_validation[selected_features]\n",
    "                \n",
    "            # Fitting the model\n",
    "            fit = model['model'].fit(X_train_sub, y_train)\n",
    "            predictions = fit.predict(X_validation_sub)\n",
    "            score = metrics.accuracy_score(y_validation, predictions)\n",
    "            print(score)\n",
    "            # Updating the best score and best model if this model is better\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = model['name']\n",
    "                best_features = selected_features\n",
    "\n",
    "print(f\"Best Model: {best_model}\\nBest Score: {best_score}\\nBest Features: {best_features}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
